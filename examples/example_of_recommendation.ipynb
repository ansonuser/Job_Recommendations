{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c62d90c-f136-4eb5-b3bc-43c4425a381c",
   "metadata": {},
   "source": [
    "## System Overview\n",
    "This system is designed to enable end-to-end machine learning workflows, integrating data collection, labeling, model fine-tuning, and prediction into a cohesive pipeline. It supports both automated and manual steps to adapt to varying degrees of supervision and task complexity.\n",
    "\n",
    "1. Data Collection\n",
    "The system begins by collecting raw data from two websites. A standardized ingestion module ensures consistent formatting, cleansing, and storage. \n",
    "\n",
    "2. Labeling\n",
    "Collected data is passed through a labeling module, which supports manual labeling through an interactive UI.\n",
    "\n",
    "3. Model Fine-tuning\n",
    "Once labeled data is available, it is used to fine-tune a LLM foundation model tailored to your interest.\n",
    "\n",
    "4. Prediction\n",
    "Our interest of range is quite small. The prediction is used to filter less interesting things out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca225a69-04a7-44b2-bb81-aa5320b22756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch\n",
    "sys.path.append(os.getcwd() +  f\"{os.sep}..\" )\n",
    "from data_collections.run import crawling \n",
    "from utils.dataset import Job, Resume\n",
    "from preprocess.loader import DataStream\n",
    "from nlp_functions.llm_utils import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d02d11-2473-44e2-832f-6b248a0c7c97",
   "metadata": {},
   "source": [
    "## Set your data up\n",
    "1. call `crawling()` to search website with words: [\"Data\", \"Scientist\", \"Machine\"] and save the raw data into your database\n",
    "2. `label_tool.py` will pop up a GUI for the user to do a binary labeling\n",
    "3. Update label to your dataset\n",
    "4. Summarize with any powerful LLM api "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0b2521-b401-499f-aacd-838529a68b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "crawling()\n",
    "# use GUI tool to label\n",
    "# $ cd JOB_RECOMMENDATION/utils\n",
    "# $ python label_tool.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710a60eb-789f-4178-b0d2-7a3a4b336e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_url = \"Url to access your collection\" # normally, the url is set to https://localhost:9200\n",
    "es = Elasticsearch(\n",
    "            [db_url],\n",
    "            basic_auth = (\"$username\", \"$password\"),\n",
    "            verify_certs=False\n",
    "            )  \n",
    "# you can access your data as follows\n",
    "respond = es.get(index=\"jobs_db\", id=\"Your_job_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd00450d-4488-4524-9f17-faf530ff28b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update yout label to database\n",
    "\n",
    "label_path = \"Path_to_your_label\" \n",
    "with open(label_path, \"r\") as f:\n",
    "    lables = json.load(f)\n",
    "\n",
    "\n",
    "for job_id, label in labels.items():\n",
    "    update_body = {\n",
    "            \"doc\" : {\n",
    "            \"Labels\": [label]\n",
    "        }\n",
    "    }\n",
    "    es.update(index=\"jobs_db\", id=job_id, body=update_body)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba30250-bcd5-4021-8b98-3202a1f10fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To utilize the power of state-of-art, use LLM API to summarize jobs' info \n",
    "# For me, it is a good way to reduce dimension and make the pattern organzied\n",
    "from openai import OpenAI\n",
    "import copy\n",
    "import time\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=\"<API-Key>\",\n",
    ")\n",
    "\n",
    "job_ids = list(labels.keys())\n",
    "train_size = 10\n",
    "data_size = 30\n",
    "summaries = {}\n",
    "for i in range(data_size):\n",
    "    print(f\"parsing row={i}\")\n",
    "    respond = es.get(index=\"jobs_db\", id=df.iloc[i,0])\n",
    "    cache = copy.copy(respond[\"_source\"])\n",
    "    del cache[\"Labels\"]\n",
    "    job = Job(**cache)\n",
    "    completion = client.chat.completions.create(\n",
    "    extra_body={},\n",
    "    model=\"deepseek/deepseek-v3-base:free\",\n",
    "    messages=[\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\":f\"\"\"\n",
    "                    You are a good headhunter. Summarize the following description:\n",
    "\n",
    "                    {job.form()}\n",
    "                    \n",
    "                    The content should be as simple as possible, including:\n",
    "                    \n",
    "                    1. what's the requirements ?\n",
    "\n",
    "                    2. what's the main task ?\n",
    "\n",
    "                    3. Salary Range \n",
    "                    \"\"\"\n",
    "        \n",
    "        }\n",
    "    ]\n",
    "    )\n",
    "    summaries[job_ids[i]] = completion.choices[0].message.content\n",
    "\n",
    "_job_ids = []\n",
    "_summaries = []\n",
    "_labels = []\n",
    "for k,v in summaries.items():\n",
    "    _job_ids.append(k)\n",
    "    _summaries.append(v)\n",
    "    _labels.append(labels[k])\n",
    "\n",
    "model_input = pd.DataFracme({\"job_id\": _job_ids, \"job_description\": _summaries, \"labels\": _labels})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee39fab5-5eb9-4d4e-a200-813c5d50f496",
   "metadata": {},
   "source": [
    "## Learning from your label\n",
    "\n",
    "1. load Mistral-7B to use in default\n",
    "2. finetunig with your label\n",
    "3. testing and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0235a94e-748c-4c8f-b96f-6e870c437c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuing\n",
    "model_path = \"Path_to_your_model\"\n",
    "agent = Agent(model_path=model_path)\n",
    "agent.load_models()\n",
    "agent.finetuning(model_input[\"job_description\"].tolist(), model_input[\"labels\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6913c31-681d-4607-b941-02428aa0c4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your test ids\n",
    "test_ids = [\n",
    "    \"Experian,Staff Software Engineer, Data & AI Platform Architecture\",\n",
    "    \"Avanade,Data & AI Solutions Architect\",\n",
    "    \"Avanade,Manager, Data Engineering\",\n",
    "    \"Samsara Inc.,Principal, Product Manager - AI Assistant\",\n",
    "    \"CrowdStrike,Sr. Software Engineer - Charlotte AI (Remote, ROU)\"\n",
    "]\n",
    "# prediction\n",
    "test_data = [summaries[test_id] for test_id in test_ids]\n",
    "prediction = agent.do_grade(test_data)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4ed92a-86f1-45ef-bd9b-22c1999d2d9b",
   "metadata": {},
   "source": [
    "output should be like:\n",
    "\n",
    "```\n",
    "array([0.8152325 , 0.8152325 , 0.83601975, 0.8587186 , 0.8601343 ],\n",
    "      dtype=float32)\n",
    "```\n",
    "\n",
    "My rank for the test data is: [2, 2, 2, 2, 1]\n",
    "\n",
    "Preidiction rank: [4, 4, 3, 2, 1]\n",
    "\n",
    "For further finetuning, change loss function to **Bayesain Personalized Ranking (BPR) Loss**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47830757-a3b1-45dd-b14b-ec7499e7defe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
